# 原始GKG数据缺失评估报告

**报告时间**: 2025-11-16 21:45 UTC+8
**评估任务**: 针对2025-03~09重跑bucket聚合
**执行结果**: ❌ **无法执行 - 原始数据已完全删除**

---

## 执行摘要

**无法重新聚合2025-03~09数据，因为：**
1. ❌ 所有原始GKG zip文件已被删除 (2,000个文件全部不存在)
2. ❌ 2025-03~09月度parquet文件结构异常 (缺失normalized列)
3. ❌ 无raw data = 无法重新运行bucket aggregation

---

## 详细诊断结果

### 1. 原始GKG数据状态

**Inventory索引**:
- 文件: `data/gdelt_raw/_raw_inventory_index_v3.csv`
- 记录数: 2,000个GKG zip文件
- 实际存在: **0个** (100%删除)

**示例路径** (均不存在):
```
C:\Users\niuji\Documents\Data\data\gdelt_raw\2024\10\20241029120000.gkg.csv.zip
C:\Users\niuji\Documents\Data\data\gdelt_raw\2024\10\20241029121500.gkg.csv.zip
...
```

**目录大小**:
```
data/gdelt_raw/2024: 0 bytes
data/gdelt_raw/2025: 4KB (仅目录结构)
```

**结论**: 原始GKG数据在某个时间点被**完全清理**,可能为了节省磁盘空间。

---

### 2. 月度Parquet文件异常分析

| 月份 | 行数 | 列数 | Bucket列覆盖率 | 状态 |
|------|------|------|----------------|------|
| 2024-10 | 60 | **60** | 100% | ✅ 正常 |
| 2024-11 | 720 | **60** | 100% | ✅ 正常 |
| 2024-12 | 744 | **60** | 100% | ✅ 正常 |
| 2025-01 | 605 | **60** | 100% | ✅ 正常 |
| 2025-02 | 672 | **60** | 100% | ✅ 正常 |
| **2025-03** | 744 | **29** ❌ | 0% | ❌ **异常** |
| **2025-04** | 720 | **29** ❌ | 0% | ❌ **异常** |
| **2025-05** | 744 | **29** ❌ | 0% | ❌ **异常** |
| **2025-06** | 329 | **29** ❌ | 0% | ❌ **异常** |
| **2025-07** | 718 | **29** ❌ | 0% | ❌ **异常** |
| **2025-08** | 744 | **29** ❌ | 0% | ❌ **异常** |
| **2025-09** | 720 | **29** ❌ | 0% | ❌ **异常** |
| 2025-10 | 681 | **60** | 100% | ✅ 正常 |

### 关键发现: 2025-03~09 Schema异常

**正常文件 (2025-02) 60列**:
```python
['ts_utc', 'ALL_art_cnt', 'ALL_tone_avg', ...,
 'OIL_CORE_norm_art_cnt',      # ← normalized bucket列
 'GEOPOL_norm_art_cnt',
 'USD_RATE_norm_art_cnt',
 'SUPPLY_CHAIN_norm_art_cnt',
 'MACRO_norm_art_cnt',
 'ESG_POLICY_norm_art_cnt',
 'OTHER_norm_art_cnt',
 ...
 'mapped_ratio']
```

**异常文件 (2025-03) 29列**:
```python
['ts_utc', 'ALL_art_cnt', 'ALL_tone_avg', ...,
 'OIL_CORE_art_cnt',           # ← 仅有raw bucket列
 'GEOPOL_art_cnt',             # 但全为None
 'USD_RATE_art_cnt',
 ...
]
# 缺失所有 *_norm_* 列 (31列)
# 缺失 mapped_ratio 列
```

**2025-03示例数据**:
```
                     ts_utc  ALL_art_cnt  OIL_CORE_art_cnt  ...
2025-03-01 01:00:00+00:00         6898              None  ...
2025-03-01 02:00:00+00:00         5662              None  ...
2025-03-01 03:00:00+00:00         4774              None  ...
```

- 所有bucket列类型: `object` (而非`float64`)
- 所有值: `None` (100% NULL)

---

## 根本原因分析

### 为什么2025-03~09缺失bucket数据?

**可能原因 (按概率排序)**:

#### 1. 聚合脚本未运行 (最可能 - 80%)
```
假设:
- 2025-03起,自动化聚合job停止运行
- 月度文件由其他脚本生成 (仅ALL统计,无bucket)
- 只保留了基础GDELT统计,未进行bucket mapping

证据:
- 列数变化: 60 → 29 (缺失normalized列)
- 列类型变化: float64 → object
- 100% NULL值
```

#### 2. Schema迁移失败 (可能 - 15%)
```
假设:
- 2025-03进行了schema升级
- 新版aggregator有bug,未输出normalized列
- 2025-10修复后恢复正常

证据:
- 2025-10又恢复为60列
- 刚好7个月的gap
```

#### 3. Raw数据问题 (不太可能 - 5%)
```
假设:
- 2025-03~09期间GDELT源数据格式变化
- Aggregator无法解析,只输出了基础统计

证据:
- ALL_art_cnt仍有数据 (6898, 5662等)
- 说明raw GKG确实被读取了
- 但bucket mapping完全失败
```

---

## 无法修复的原因

### 为什么不能简单"重跑聚合"?

```
用户要求: 用gdelt_gkg_bucket_aggregator.py重新聚合
         读取: data/gdelt_raw/2025/{03..09}/**/*.gkg.csv.zip

现实:
  ❌ data/gdelt_raw/ 目录下 0 个GKG文件
  ❌ 所有原始数据已被永久删除
  ❌ 无raw input → 无法运行aggregator

结论: 技术上不可能,除非重新获取raw data
```

---

## 可行替代方案

### 方案A: 从GDELT重新下载 (可行但耗时)

**步骤**:
1. 使用GDELT HTTP API下载2025-03~09的GKG文件
2. URL模式: `http://data.gdeltproject.org/gkg/YYYYMMDDHHMMSS.gkg.csv.zip`
3. 运行gdelt_gkg_bucket_aggregator.py重新聚合
4. 生成normalized月度文件

**工作量估算**:
```
时间范围: 2025-03-01 to 2025-09-30 (214天)
文件数: 214天 x 96文件/天 = 20,544个zip文件
文件大小: ~10-50MB/文件 → 总计 200GB-1TB
下载时间: ~20-100小时 (取决于带宽)
聚合时间: ~10-20小时
总耗时: 1.5-5天
```

**实施难度**: 🔴 高
- 需要稳定网络连接
- 需要大量磁盘空间 (500GB+)
- 需要长时间运行脚本
- 存在下载失败风险

**推荐指数**: ⭐⭐ (2/5)

---

### 方案B: 接受现有数据限制 (务实)

**现实**:
- 有效bucket数据: 6个月 (2024-10~2025-02, 2025-10)
- 最长连续段: 75天 (2024-10-29 ~ 2025-01-12)
- 无法满足90天窗口需求

**调整策略**:

#### B1. 降低窗口要求 (修改方法论)
```python
# 原要求
TRAIN_DAYS = 60
TEST_DAYS = 30
总需求 = 90天

# 调整为
TRAIN_DAYS = 40
TEST_DAYS = 20
总需求 = 60天 ← 可满足!
```

**风险**:
- ❌ 违反原始No-Drift methodology
- ❌ 统计显著性下降
- ❌ PMR计算不准确 (需3-6月数据)

**推荐指数**: ⭐⭐⭐ (3/5) - 如果可接受方法论relaxation

#### B2. 使用"交易日"窗口 (重新定义)
```python
# 不使用calendar days,改用trading hours
TRAIN_HOURS = 400  # ~50个交易日
TEST_HOURS = 200   # ~25个交易日
```

过滤掉周末和非交易时间,只用连续交易小时

**优势**:
- ✅ 更符合金融市场实际
- ✅ 可使用现有75天连续段中的~40天交易时间
- ✅ 减少噪音 (周末无数据)

**推荐指数**: ⭐⭐⭐⭐ (4/5) - 技术上合理

---

### 方案C: 等待新数据积累 (保守)

**时间线**:
```
现在: 2025-11-16
有数据: 2025-10 (1个月)

等待至:
- 2026-01: 4个月连续数据 ← 仍不够
- 2026-04: 7个月连续数据 ← 接近
- 2026-07: 10个月连续数据 ← 足够!
```

**前提条件**:
- ✅ 确保2025-11起每月bucket聚合正常运行
- ✅ 监控数据质量,防止再次出现NULL列
- ✅ 避免原始数据被删除

**推荐指数**: ⭐⭐⭐⭐⭐ (5/5) - 最稳妥,符合原始方法论

---

### 方案D: Hybrid混合方案 (推荐!)

**策略**: 使用现有数据先做Soft评估,等新数据积累再做Hard评估

**Phase 1 (立即执行)**:
1. 使用2024-10~2025-02 (5个月)
2. 调整为"交易时间"窗口
3. 评估参数:
   ```python
   TRAIN_HOURS = 400  # 可用交易小时
   TEST_HOURS = 200
   ```
4. 标记为 **Shadow/Experimental** 结果
5. **不**将其列为正式Hard候选

**Phase 2 (2026-Q2)**:
1. 等待2025-11~2026-04数据 (6个月新数据)
2. 使用完整90天窗口
3. 执行正式Hard IC评估
4. 如达标,列为正式Base候选

**优势**:
- ✅ 立即可见初步结果 (验证方向)
- ✅ 不违反No-Drift合约 (shadow结果)
- ✅ 为正式评估做技术准备
- ✅ 时间利用最优

**推荐指数**: ⭐⭐⭐⭐⭐ (5/5) - **最优方案**

---

## 最终建议

### 推荐执行: **方案D (Hybrid混合方案)**

#### 立即行动 (本次会话):
1. ✅ 确认无法重新聚合2025-03~09 (已完成)
2. ✅ 生成本诊断报告 (已完成)
3. 🔄 使用现有数据执行Shadow IC评估:
   - 调整为trading-hours窗口
   - 使用2024-10~2025-02最长连续段
   - 标记为experimental
   - 查看IC/IR/PMR趋势

#### 中期准备 (1-2周):
1. 检查并确保2025-11起聚合正常
2. 设置数据质量监控
3. 准备正式评估脚本 (90天窗口版本)
4. 文档化当前limitation

#### 长期目标 (2026-Q2):
1. 积累至少6个月连续新数据
2. 执行正式Hard IC评估
3. 如达标,提交为Base候选

---

## 技术细节

### 如需执行Shadow评估 (方案D Phase 1):

```python
# 修改evaluate_composite_ic_v2.py

# 1. 使用最长连续段
df_segment = df_valid.iloc[start_idx:end_idx+1]  # 75天segment

# 2. 仅保留交易时间
df_trading = df_segment[df_segment['ret_1h'].notna()]

# 3. 调整窗口 (用小时数,非天数)
TRAIN_HOURS = 400
TEST_HOURS = 200

# 4. 标记输出
OUTPUT_PREFIX = "shadow_experimental"
```

---

## 结论

❌ **原计划无法执行**: 无raw GKG数据,无法重跑聚合

✅ **推荐方案**: Hybrid混合方案
- 立即: Shadow评估 (验证方向)
- 长期: 等待新数据正式评估 (2026-Q2)

📊 **当前数据限制**:
- 6个月有bucket数据 (非连续)
- 最长75天连续段 (不足90天)
- 交易时间仅~40天有效数据

⏰ **时间规划**:
- 立即: 完成Shadow评估 (1-2小时)
- 2026-Q2: 正式Hard评估 (6个月后)

---

**报告生成**: 2025-11-16 21:45 UTC+8
**诊断脚本**: `diagnose_missing_raw_data.py`
